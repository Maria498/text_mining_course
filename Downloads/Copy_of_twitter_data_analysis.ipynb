{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJMnlUMtjYMG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import math\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('names')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_duplicates = pd.read_csv('/content/sample_data/Twitter_data.csv')\n",
        "df = df_with_duplicates.drop_duplicates(['twitter_screen_name'])\n",
        "print(f\"Length of df_with_duplicates: {len(df_with_duplicates)}\")\n",
        "print(f\"Length of df: {len(df)}\")"
      ],
      "metadata": {
        "id": "ecyB9YhcyVD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c155723d-8d25-4e87-dcbd-192bb0371bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of df_with_duplicates: 29063\n",
            "Length of df: 26585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "pbXs5Yb0_v_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_out_non_english_stop_lemma_words(text, lemmatize):\n",
        "    # tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # filter out non-English and stop  words\n",
        "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
        "    filtered_words = [word.lower() for word in words if word.lower() in english_vocab]\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    final_words = [word for word in filtered_words if word not in stopwords_set]\n",
        "\n",
        "    # lemmatization based on true/false flag\n",
        "    if lemmatize:\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      final_words = [lemmatizer.lemmatize(word) for word in final_words]\n",
        "\n",
        "    return final_words\n",
        "\n",
        "\n",
        "def calculate_word_frequencies(text, top_n=30):\n",
        "    # clean non-english, stop words and lemmatize\n",
        "    words = filter_out_non_english_stop_lemma_words(text, True)\n",
        "\n",
        "    # calc word frequencies using FreqDist\n",
        "    freq_dist = FreqDist(words)\n",
        "    top_n_words = freq_dist.most_common(top_n)\n",
        "\n",
        "    return top_n_words\n",
        "\n",
        "\n",
        "def identify_gender_by_first_name(first_name):\n",
        "    # remove non-alphabetic characters\n",
        "    first_name_lower = re.sub(r'[^a-zA-Z]', '', first_name.lower())\n",
        "    if first_name_lower in male_names and first_name_lower not in female_names:\n",
        "        return 'Male'\n",
        "    elif first_name_lower in female_names and first_name_lower not in male_names:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def calculate_pmi(text, pr_class, total_words):\n",
        "    words = filter_out_non_english_stop_lemma_words(text, False)\n",
        "\n",
        "    # calc word frequencies\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # calc PMI for each word\n",
        "    pmi_words = []\n",
        "    for word, freq in word_freq.items():\n",
        "        pr_word_class = freq / total_words\n",
        "        pr_word = len([1 for doc in new_df['twitter_desc'] if word in doc.lower()]) / total_words\n",
        "\n",
        "        # avoid zeroes before division\n",
        "        if pr_word != 0:\n",
        "            pmi = math.log2(pr_word_class / (pr_word * pr_class))\n",
        "            pmi_words.append((word, pmi))\n",
        "\n",
        "    # sort by PMI in desc order\n",
        "    pmi_words.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return pmi_words[:30]\n",
        "\n",
        "def calculate_pmi_for_type(df, type_column, type_value):\n",
        "    type_df = df[df[type_column] == type_value]\n",
        "    non_type_df = df[df[type_column] != type_value]\n",
        "\n",
        "    # concatenate the Twitter descriptions for each group\n",
        "    type_desc = ' '.join(type_df['twitter_desc'].dropna())\n",
        "    non_type_desc = ' '.join(non_type_df['twitter_desc'].dropna())\n",
        "\n",
        "    # calculate total words for PMI calculation\n",
        "    total_type_words = len(word_tokenize(type_desc))\n",
        "    total_non_type_words = len(word_tokenize(non_type_desc))\n",
        "\n",
        "    # calculate PMI for type users\n",
        "    pr_type = type_df.shape[0] / df.shape[0]\n",
        "    type_pmi_words = calculate_pmi(type_desc, pr_type, total_type_words)\n",
        "\n",
        "    # calculate PMI for non-type users\n",
        "    pr_non_type = non_type_df.shape[0] / df.shape[0]\n",
        "    non_type_pmi_words = calculate_pmi(non_type_desc, pr_non_type, total_non_type_words)\n",
        "\n",
        "    return type_pmi_words, non_type_pmi_words\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uSekhTfm_1Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Top 30 most frequent words in 'twitter_desc' column"
      ],
      "metadata": {
        "id": "e4mxe8xyzaWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_desc = ' '.join(df['twitter_desc'].dropna())\n",
        "top_words = calculate_word_frequencies(twitter_desc)\n",
        "\n",
        "print(\"Top 30 most frequent words in 'twitter_desc' column:\")\n",
        "for word, frequency in top_words:\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "id": "8O_SCvPxk5iR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84425d6-3cab-4225-dadd-0982a18e13ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 most frequent words in 'twitter_desc' column:\n",
            "official: 2741\n",
            "twitter: 2344\n",
            "news: 1784\n",
            "new: 1423\n",
            "world: 1384\n",
            "account: 1343\n",
            "author: 1143\n",
            "u: 966\n",
            "de: 826\n",
            "follow: 731\n",
            "former: 669\n",
            "actor: 661\n",
            "music: 649\n",
            "time: 595\n",
            "writer: 570\n",
            "life: 554\n",
            "host: 552\n",
            "la: 530\n",
            "love: 529\n",
            "sport: 516\n",
            "page: 511\n",
            "people: 506\n",
            "contact: 488\n",
            "father: 488\n",
            "husband: 486\n",
            "since: 471\n",
            "business: 464\n",
            "album: 457\n",
            "champion: 452\n",
            "best: 448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Words associated with gender"
      ],
      "metadata": {
        "id": "By4HTfXRzcHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# female, male name lexicons\n",
        "names = nltk.corpus.names\n",
        "male_names = set(name.lower() for name in names.words('male.txt'))\n",
        "female_names = set(name.lower() for name in names.words('female.txt'))\n",
        "\n",
        "# copy of the df for Person type (filter out org, work and so on)\n",
        "df_person = df[df['type2'] == 'Person'].copy()\n",
        "df_person['gender'] = df_person['twitter_name'].apply(lambda x: identify_gender_by_first_name(x.split()[0]) if pd.notnull(x) else 'Unknown')\n",
        "\n",
        "# create a new df with selected columns, remove NaN values from 'twitter_desc'\n",
        "new_df = df_person[['twitter_name', 'gender', 'twitter_desc']].copy()\n",
        "new_df['twitter_desc'].fillna('', inplace=True)\n",
        "new_df = new_df[new_df['gender'] != 'Unknown']\n",
        "\n",
        "#new_df.to_csv('/content/sample_data/Twitter_data_with_gender.csv', index=False)\n",
        "\n",
        "male_count = new_df[new_df['gender'] == 'Male'].shape[0]\n",
        "female_count = new_df[new_df['gender'] == 'Female'].shape[0]\n",
        "\n",
        "print(f\"Number of Male users: {male_count}\")\n",
        "print(f\"Number of Female users: {female_count}\")\n",
        "\n",
        "word_freq_by_gender = new_df.groupby('gender')['twitter_desc'].apply(lambda x: calculate_word_frequencies(' '.join(x)))\n",
        "\n",
        "# results\n",
        "for gender, top_words in word_freq_by_gender.items():\n",
        "    print(f\"\\nTop 30 most frequent words for {gender} users:\")\n",
        "    for word, frequency in top_words:\n",
        "        print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYHdnvkxz2Hn",
        "outputId": "e14cb926-96e0-4484-90e9-b23c232ca936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Male users: 5895\n",
            "Number of Female users: 3356\n",
            "\n",
            "Top 30 most frequent words for Female users:\n",
            "author: 322\n",
            "actress: 248\n",
            "new: 195\n",
            "writer: 159\n",
            "wife: 142\n",
            "official: 137\n",
            "host: 126\n",
            "world: 126\n",
            "former: 123\n",
            "lover: 119\n",
            "twitter: 119\n",
            "actor: 117\n",
            "mother: 112\n",
            "time: 112\n",
            "de: 102\n",
            "love: 99\n",
            "book: 94\n",
            "life: 90\n",
            "founder: 83\n",
            "la: 81\n",
            "director: 80\n",
            "singer: 79\n",
            "girl: 77\n",
            "news: 75\n",
            "producer: 72\n",
            "champion: 70\n",
            "speaker: 66\n",
            "contact: 61\n",
            "correspondent: 61\n",
            "music: 59\n",
            "\n",
            "Top 30 most frequent words for Male users:\n",
            "author: 450\n",
            "official: 400\n",
            "twitter: 349\n",
            "new: 346\n",
            "husband: 293\n",
            "father: 286\n",
            "actor: 278\n",
            "former: 278\n",
            "host: 210\n",
            "de: 197\n",
            "writer: 196\n",
            "world: 191\n",
            "time: 177\n",
            "account: 166\n",
            "player: 159\n",
            "champion: 157\n",
            "director: 152\n",
            "dad: 143\n",
            "founder: 142\n",
            "life: 131\n",
            "music: 126\n",
            "book: 126\n",
            "love: 122\n",
            "producer: 115\n",
            "professional: 114\n",
            "page: 110\n",
            "editor: 108\n",
            "la: 104\n",
            "contact: 100\n",
            "business: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "- 'author' is the most frequent word in both groups, what might be influensed by self-promoting, professional career and other factors\n",
        "- in both groups words associated with personal and familial roles are at the top of the list (presence of societal biases and gender stereotypes)\n",
        "- both groups also mention words related to professional activities, like writing, producing, or engaging with media\n",
        "\n"
      ],
      "metadata": {
        "id": "1CifRlWcowNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Words associated with gender using PMI"
      ],
      "metadata": {
        "id": "uvfmaTG_lQI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(word_tokenize(' '.join(new_df['twitter_desc'])))\n",
        "\n",
        "# calculate PMI for female users\n",
        "pr_female = female_count / (male_count + female_count)\n",
        "female_pmi_words = calculate_pmi(' '.join(new_df[new_df['gender'] == 'Female']['twitter_desc']), pr_female, total_words)\n",
        "\n",
        "# calculate PMI for male users\n",
        "pr_male = male_count / (male_count + female_count)\n",
        "male_pmi_words = calculate_pmi(' '.join(new_df[new_df['gender'] == 'Male']['twitter_desc']), pr_male, total_words)\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for Female users:\")\n",
        "for word, pmi in female_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for Male users:\")\n",
        "for word, pmi in male_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")\n"
      ],
      "metadata": {
        "id": "qCwiRWvvoytb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70fa263-fb68-4dc4-ffe9-81879e53c230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 30 most frequent words with highest PMI for Female users:\n",
            "blah: 2.7847947033399807\n",
            "geordie: 2.462866608452618\n",
            "mutton: 2.462866608452618\n",
            "usun: 2.462866608452618\n",
            "winehouse: 2.462866608452618\n",
            "flea: 2.462866608452618\n",
            "optimism: 2.462866608452618\n",
            "impressionist: 2.462866608452618\n",
            "lunar: 2.462866608452618\n",
            "tongue: 2.462866608452618\n",
            "shetland: 2.462866608452618\n",
            "patsy: 2.462866608452618\n",
            "disability: 2.462866608452618\n",
            "bittersweet: 2.0478291091737746\n",
            "latex: 2.0478291091737746\n",
            "stardom: 2.0478291091737746\n",
            "decision: 2.0478291091737746\n",
            "algorithmic: 2.0478291091737746\n",
            "ballet: 1.7847947033399805\n",
            "meditation: 1.7847947033399805\n",
            "administration: 1.7259010142864122\n",
            "eyewitness: 1.4628666084526183\n",
            "trucker: 1.462866608452618\n",
            "spectacular: 1.462866608452618\n",
            "beautifully: 1.462866608452618\n",
            "pursuer: 1.462866608452618\n",
            "melanin: 1.462866608452618\n",
            "weirdness: 1.462866608452618\n",
            "unprofessionally: 1.462866608452618\n",
            "eyebrow: 1.462866608452618\n",
            "\n",
            "Top 30 most frequent words with highest PMI for Male users:\n",
            "wheeled: 2.2350800117464726\n",
            "honk: 1.6501175110253166\n",
            "bison: 1.6501175110253166\n",
            "bermuda: 1.6501175110253166\n",
            "psychic: 1.6501175110253166\n",
            "batch: 1.6501175110253166\n",
            "luigi: 1.6501175110253166\n",
            "perfecting: 1.6501175110253166\n",
            "canty: 1.6501175110253166\n",
            "dwight: 1.6501175110253166\n",
            "sixteen: 1.6501175110253166\n",
            "untrue: 1.6501175110253166\n",
            "guarantee: 1.6501175110253166\n",
            "disturbed: 1.6501175110253166\n",
            "parish: 1.6501175110253166\n",
            "brewmaster: 1.6501175110253166\n",
            "attention: 1.3870831051915224\n",
            "nicolas: 1.2350800117464729\n",
            "ministry: 1.2350800117464729\n",
            "starred: 1.2350800117464729\n",
            "warcraft: 1.2350800117464729\n",
            "donovan: 1.2350800117464729\n",
            "opposition: 1.2350800117464729\n",
            "superpower: 1.2350800117464729\n",
            "episcopal: 1.2350800117464729\n",
            "derek: 1.1355443381955583\n",
            "pablo: 1.0651550103041603\n",
            "bloody: 1.0651550103041603\n",
            "palmer: 0.9720456059126787\n",
            "lightweight: 0.8725099323617643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "\n",
        "- we've got a totally different results with PMI that without as in previous frequency count. We saw that if rare word occurs in the context of a specific class (for example 'blah' among female users and 'wheeled' among male class), and this co-occurrence is much higher than expected by chance, the PMI score for that word in specific class will be high.\n",
        "- for the female users, some of the words with high PMI values include 'blah', 'geordie', 'mutton', 'winehouse', 'optimism', 'impressionist', and 'lunar'.\n",
        "- similarly, for the male users, words like 'wheeled', 'honk', 'bermuda', 'batch', 'luigi', and 'canty' have high PMI values, strongly associated with male users.\n",
        "- these results indicate the presence of certain language patterns or topics that are more prevalent or distinctive among male and female users based on our dataset."
      ],
      "metadata": {
        "id": "IdN3HPQC8gFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. MusicalArtist Politician most frequent PMI associated words"
      ],
      "metadata": {
        "id": "eoUUFh86A3Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calc PMI for MusicalArtist users\n",
        "musical_artist_pmi_words, non_musical_artist_pmi_words = calculate_pmi_for_type(df, 'type1', 'MusicalArtist')\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for MusicalArtist users:\")\n",
        "for word, pmi in musical_artist_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for Non-MusicalArtist users:\")\n",
        "for word, pmi in non_musical_artist_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")"
      ],
      "metadata": {
        "id": "QAuqWr0N8iUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55508c85-0c6c-4730-af28-6353b7b23a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 30 most frequent words with highest PMI for MusicalArtist users:\n",
            "electronic: 5.276680751921932\n",
            "melody: 5.276680751921932\n",
            "nuclear: 4.954752657034569\n",
            "blast: 4.954752657034569\n",
            "donnie: 4.954752657034569\n",
            "anniversary: 4.762107579092173\n",
            "royalty: 4.539715157755725\n",
            "bluegrass: 4.539715157755725\n",
            "sonic: 4.539715157755725\n",
            "vibrant: 4.539715157755725\n",
            "vinyl: 4.276680751921932\n",
            "dove: 4.276680751921932\n",
            "stellar: 4.276680751921932\n",
            "acoustic: 4.177145078371017\n",
            "debut: 3.954752657034569\n",
            "junior: 3.954752657034569\n",
            "daydreamer: 3.954752657034569\n",
            "label: 3.954752657034569\n",
            "outrageous: 3.954752657034569\n",
            "jorge: 3.954752657034569\n",
            "honk: 3.954752657034569\n",
            "slang: 3.954752657034569\n",
            "steady: 3.954752657034569\n",
            "empire: 3.954752657034569\n",
            "trivium: 3.954752657034569\n",
            "revolt: 3.954752657034569\n",
            "flamboyant: 3.954752657034569\n",
            "hairline: 3.954752657034569\n",
            "veil: 3.954752657034569\n",
            "winehouse: 3.954752657034569\n",
            "\n",
            "Top 30 most frequent words with highest PMI for Non-MusicalArtist users:\n",
            "affiliate: 6.967410729518251\n",
            "privacy: 5.521154499628687\n",
            "analysis: 5.3691514061836365\n",
            "nonprofit: 5.348973524246007\n",
            "customer: 5.336729928491259\n",
            "assistance: 5.2866892459916635\n",
            "millions: 5.199226404741324\n",
            "android: 5.199226404741324\n",
            "advancing: 5.199226404741324\n",
            "association: 5.176506328241241\n",
            "coverage: 5.073695522657466\n",
            "alternative: 5.006581326798928\n",
            "contemporary: 4.899666122882417\n",
            "newspaper: 4.862191417463754\n",
            "nuclear: 4.843082594516049\n",
            "service: 4.82566554143864\n",
            "industry: 4.78418890546248\n",
            "advance: 4.78418890546248\n",
            "brought: 4.722788360798337\n",
            "affordable: 4.658658023378622\n",
            "manufacturer: 4.658658023378622\n",
            "newsroom: 4.625491159443422\n",
            "across: 4.625491159443422\n",
            "delivery: 4.591543827520085\n",
            "nonpartisan: 4.591543827520085\n",
            "destination: 4.556778409359408\n",
            "membership: 4.521154499628687\n",
            "campus: 4.521154499628687\n",
            "leading: 4.511504329594968\n",
            "weather: 4.4907808505851685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "- the results are logical based on the context of the user groups being analyzed\n",
        "- for MusicalArtist users words like \"electronic\", \"melody\", \"acoustic\", \"royalty\" are highly relevant to the music industry, indicating a strong association with MusicalArtist users\n",
        "- for Non-MusicalArtist users words such as \"affiliate\", \"privacy\", \"analysis\", \"customer\" are more relevant to business, technology, and organizational topics.\n",
        "- overall results align with the expected differences in the topics and interests between MusicalArtist and Non-MusicalArtist users on Twitter"
      ],
      "metadata": {
        "id": "xH-U_DYQI8xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate PMI for Politician users\n",
        "politician_pmi_words, non_politician_pmi_words = calculate_pmi_for_type(df, 'type1', 'Politician')\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for Politician users:\")\n",
        "for word, pmi in politician_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")\n",
        "\n",
        "print(\"\\nTop 30 most frequent words with highest PMI for Non-Politician users:\")\n",
        "for word, pmi in non_politician_pmi_words:\n",
        "    print(f\"{word}: {pmi}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7dpPQhaI9vW",
        "outputId": "06f99479-3319-4821-bdae-f09643540ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 30 most frequent words with highest PMI for Politician users:\n",
            "pueblo: 6.5159304925656745\n",
            "hampshire: 6.5159304925656745\n",
            "congressional: 6.265952239557327\n",
            "administration: 6.194002397678312\n",
            "constituency: 6.1008929932868305\n",
            "opposition: 6.1008929932868305\n",
            "estado: 6.1008929932868305\n",
            "agriculture: 6.1008929932868305\n",
            "assistance: 6.1008929932868305\n",
            "hiker: 6.1008929932868305\n",
            "whip: 5.930967991844518\n",
            "district: 5.8944421158194045\n",
            "serving: 5.882712823237298\n",
            "proudly: 5.869567447180375\n",
            "vermont: 5.837858587453037\n",
            "subcommittee: 5.778964898399468\n",
            "minister: 5.76831765419996\n",
            "pennsylvania: 5.738322913902122\n",
            "senator: 5.72614819995602\n",
            "congressman: 5.6154661661165886\n",
            "governor: 5.531527347616693\n",
            "turkey: 5.5159304925656745\n",
            "secretary: 5.5159304925656745\n",
            "counselor: 5.5159304925656745\n",
            "infrastructure: 5.5159304925656745\n",
            "islamic: 5.5159304925656745\n",
            "kansan: 5.5159304925656745\n",
            "chairwoman: 5.5159304925656745\n",
            "usun: 5.5159304925656745\n",
            "tammy: 5.5159304925656745\n",
            "\n",
            "Top 30 most frequent words with highest PMI for Non-Politician users:\n",
            "affiliate: 6.832662507914278\n",
            "privacy: 5.312405696580937\n",
            "analysis: 5.234403184579664\n",
            "customer: 5.193761200082318\n",
            "nonprofit: 5.193761200082318\n",
            "android: 5.108872302495805\n",
            "millions: 5.064478183137352\n",
            "association: 5.0417581066372685\n",
            "assistance: 5.018674493524227\n",
            "advancing: 5.018674493524227\n",
            "coverage: 4.955249113382593\n",
            "nuclear: 4.922459178264924\n",
            "alternative: 4.8718331051949555\n",
            "contemporary: 4.7649179012784435\n",
            "newspaper: 4.727443195859781\n",
            "service: 4.702552020318071\n",
            "industry: 4.66439102532448\n",
            "advance: 4.649440683858508\n",
            "across: 4.523909801774649\n",
            "brought: 4.523909801774649\n",
            "affordable: 4.523909801774649\n",
            "manufacturer: 4.523909801774649\n",
            "newsroom: 4.49074293783945\n",
            "destination: 4.49074293783945\n",
            "electronic: 4.4567956059161125\n",
            "membership: 4.4567956059161125\n",
            "delivery: 4.4567956059161125\n",
            "nonpartisan: 4.4567956059161125\n",
            "iconic: 4.386406278024714\n",
            "campus: 4.386406278024714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "- results seems reasonable, sinse most frequent words  are often related to political contexts. Words like \"pueblo,\" \"hampshire,\" \"congressional,\" \"administration,\" and \"constituency\" got a high PMI values, indicating a strong association with the Politician users in the dataset."
      ],
      "metadata": {
        "id": "RddFko4HL7G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sentiment analysis"
      ],
      "metadata": {
        "id": "zz7WhXg3NpFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Intensity Analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# replace NaN values in twitter_desc\n",
        "df_copy = df.copy()\n",
        "df_copy['twitter_desc'].fillna('', inplace=True)\n",
        "\n",
        "# calculate sentiment scores for each twitter_desc\n",
        "df_copy['sentiment_score'] = df_copy['twitter_desc'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "top_positive_users = df_copy.nlargest(10, 'sentiment_score')[['twitter_screen_name', 'sentiment_score', 'twitter_desc']]\n",
        "print(\"\\nTop 10 Most Positive Users:\")\n",
        "print(top_positive_users)\n",
        "\n",
        "top_negative_users = df_copy.nsmallest(10, 'sentiment_score')[['twitter_screen_name', 'sentiment_score', 'twitter_desc']]\n",
        "print(\"\\nTop 10 Most Negative Users:\")\n",
        "print(top_negative_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa-yQNFgMVo0",
        "outputId": "d486697a-1f6d-4eb5-9f98-8e9e56e1f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Most Positive Users:\n",
            "      twitter_screen_name  sentiment_score  \\\n",
            "18496     JasonCrabbMusic           0.9859   \n",
            "6986       IamWendyRaquel           0.9800   \n",
            "18250         mousasi_mma           0.9785   \n",
            "14603        LatitudeFest           0.9778   \n",
            "7160       malillanymarin           0.9734   \n",
            "21818           RLLracing           0.9725   \n",
            "14255     StarburyMarbury           0.9718   \n",
            "17410            petatodd           0.9716   \n",
            "13004            streamys           0.9704   \n",
            "9493            MooreMaya           0.9698   \n",
            "\n",
            "                                            twitter_desc  \n",
            "18496  HUSBAND. FATHER. CHILD OF GOD. LOVE PEOPLE. LO...  \n",
            "6986   Lover of Life, Love and Laughter! Check out my...  \n",
            "18250  *BELLATOR MW CHAMPION 2X\\n*STRIKEFORCE LHW CHA...  \n",
            "14603  Winner of the UK Festival Awards 'Best Major F...  \n",
            "7160   http://Actress . TV host & model . Animal Righ...  \n",
            "21818  1992 #IndyCar champion, 2004 #Indy500 winner f...  \n",
            "14255  JESUS IS REAL! God Is Love, Love Is God.... LO...  \n",
            "17410  Former page3 gal. Yes. Lanky. Yes. Mum. Yes. T...  \n",
            "13004  The Streamy Awards, honoring the best in onlin...  \n",
            "9493   Basketball player, daughter, & drummer. Friend...  \n",
            "\n",
            "Top 10 Most Negative Users:\n",
            "      twitter_screen_name  sentiment_score  \\\n",
            "24232      GraigKreindler          -0.9559   \n",
            "14406          garywhitta          -0.9534   \n",
            "12915           LisaBloom          -0.9485   \n",
            "14294        helenprejean          -0.9468   \n",
            "20266       TonyAtamanuik          -0.9260   \n",
            "24031          andydiggle          -0.9231   \n",
            "10750        SharonHorgan          -0.9217   \n",
            "13020              NCA_UK          -0.9217   \n",
            "25450          Bobosphere          -0.9217   \n",
            "20204          DavidGrann          -0.9186   \n",
            "\n",
            "                                            twitter_desc  \n",
            "24232  Graig paints dead baseball players. Though not...  \n",
            "14406  Writer of things. ROGUE ONE: A STAR WARS STORY...  \n",
            "12915  Trial lawyer fighting for victims of discrimin...  \n",
            "14294  Life-lover, anti-death penalty activist, spiri...  \n",
            "20266  Iâ€™m a person in my house. I cook my food. I ea...  \n",
            "24031  Writer of PROMETHEE 13:13, JAMES BOND, THIEF O...  \n",
            "10750  Angelo's, Pulling, Dead Boss, Bad Sugar, Catas...  \n",
            "13020  National Crime Agency. Leading the UK's fight ...  \n",
            "25450  Crisis actor. Globalist. Enemy of the People. ...  \n",
            "20204  Staff Writer @NewYorker. Author of The White D...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "- moatly the results appear to be reasonable based on the sentiment scores calculated using vader_lexicon\n",
        "- top negative description really looks negative, example: GraigKreindler\tGraig Kreindler\tGraig paints dead baseball players. Though not OF them dead. Not that there's anything wrong with the dead. In fact, Graig loves the Dead.\n",
        "- top positive description looks positive, example: JasonCrabbMusic\tJason Crabb\tHUSBAND. FATHER. CHILD OF GOD. LOVE PEOPLE. LOVE JESUS. LOVE IS STRONGER! @TheGRAMMYs award winner. :-)\n",
        "- sentiment analysis might struggle with nuanced language due to the lack of context. For example, user in the 4th place of negative descriptions identifies himself as a 'life-lover,' while terms like 'anti-death penalty activist' and 'spiritual adviser to men and women on death row' suggest involvement with heavy and potentially negative topics. These complex sentiments not fully captured by the algorithm which places the user at the top of negative sentiments.\n"
      ],
      "metadata": {
        "id": "c8qfFM9VQgfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Hebrew, arabic characters in twitter_desc"
      ],
      "metadata": {
        "id": "VJpofQ2QSKnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def percentage_of_words_in_script(text, script_regex):\n",
        "    # tokenize the text into words\n",
        "    words = nltk.word_tokenize(str(text))\n",
        "\n",
        "    # count the words containing characters in the specified script\n",
        "    script_words_count = sum(1 for word in words if re.search(script_regex, word))\n",
        "\n",
        "    # calculate the percentage\n",
        "    percentage = (script_words_count / len(words)) * 100\n",
        "\n",
        "    return percentage\n",
        "\n",
        "# calculate the percentage of words containing Hebrew characters\n",
        "hebrew_percentage = percentage_of_words_in_script(df['twitter_desc'].str.lower(), '[\\u0590-\\u05FF]')\n",
        "print(f\"The percentage of words containing Hebrew characters: {hebrew_percentage:.5f}%\")\n",
        "\n",
        "# calculate the percentage of words containing Arabic characters\n",
        "arabic_percentage = percentage_of_words_in_script(df['twitter_desc'].str.lower(), '[\\u0600-\\u06FF]')\n",
        "print(f\"The percentage of words containing Arabic characters: {arabic_percentage:.5f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdFG0kOZQhne",
        "outputId": "97ab911b-79e4-4a2b-af46-bed96c92a855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The percentage of words containing Hebrew characters: 0.00000%\n",
            "The percentage of words containing Arabic characters: 0.00000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Emoticons"
      ],
      "metadata": {
        "id": "UIsnwdG5WAe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emoticons(text):\n",
        "    emoticon_pattern = r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)'\n",
        "    return re.findall(emoticon_pattern, text)\n",
        "\n",
        "# Extract emoticons from 'twitter_desc' column and flatten the list\n",
        "all_emoticons = df['twitter_desc'].apply(extract_emoticons).explode()\n",
        "\n",
        "# Count the occurrences of each emoticon\n",
        "emoticon_counts = Counter(all_emoticons)\n",
        "\n",
        "# Print the top 10 most frequent emoticons\n",
        "top_emoticons = emoticon_counts.most_common(10)\n",
        "print(\"Top 10 Most Frequent Emoticons:\")\n",
        "for emoticon, count in top_emoticons:\n",
        "    print(f\"{emoticon}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNU6zzMJWEtJ",
        "outputId": "9fbbebcc-6fa3-4320-f934-5cf73feaee13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Frequent Emoticons:\n",
            "nan: 26490\n",
            ":): 57\n",
            ";): 11\n",
            ":D: 9\n",
            ":-): 7\n",
            ":P: 7\n",
            ";-): 4\n",
            ":(: 2\n",
            ";P: 1\n"
          ]
        }
      ]
    }
  ]
}